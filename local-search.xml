<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>How I Built the AirBrB with ReactJS</title>
    <link href="/2024/09/23/airbrb/"/>
    <url>/2024/09/23/airbrb/</url>
    
    <content type="html"><![CDATA[<p><strong>Introduction</strong><br>AirBrB is a property rental platform inspired by Airbnb, allowing users to list, search, and book properties. In this project, I focused on building the frontend using ReactJS. The goal was to create a responsive single-page application (SPA) that interacts with a backend API, offering features like user authentication, property listing management, booking capabilities, and advanced search filters.</p><p><strong>Project Overview</strong><br><strong>1. User Authentication</strong><br>The platform supports user authentication, allowing users to register, log in, and log out. Once logged in, users can access additional features like creating and managing listings, as well as booking properties.</p><p>Login Page: Allows users to input their email and password.<br>Registration Page: Users provide their name, email, password, and confirm their password.<br>Logout Functionality: The logout button is available on all pages when the user is logged in.</p><img src="/2024/09/23/airbrb/login.png" class title="login"><img src="/2024/09/23/airbrb/logout.png" class title="logout"><img src="/2024/09/23/airbrb/loginc.png" class title="login"><img src="/2024/09/23/airbrb/regis.png" class title="regist"><p><strong>2. Creating &amp; Managing Listings</strong><br>Once logged in, users (hosts) can create property listings. They can manage their listings through a dashboard, where they can edit or delete existing properties. Listings become visible to other users once published.</p><p>Hosted Listings Screen: Displays all listings created by the user, showing details like property type, number of beds, price, etc.<br>Create Listing: Allows users to provide details such as title, address, price, and amenities.<br>Edit Listing: Users can update their existing listings.<br>Delete Listing: Users can remove listings they no longer want to display.</p><img src="/2024/09/23/airbrb/publish.png" class title="publish"><img src="/2024/09/23/airbrb/inv.png" class title="inv"><p><strong>3. Viewing &amp; Searching Listings</strong><br>All users, whether logged in or not, can browse property listings. The platform provides filters to help narrow down results based on user preferences, such as price, number of bedrooms, and availability.</p><p>Listings Screen: Displays all available (published) listings, showing the title, price, images, and reviews.<br>Search Filters: Users can filter listings based on title, city, price range, number of bedrooms, and availability dates.<br>Sorting: Users can sort listings by review ratings, making it easier to find highly-rated properties.</p><img src="/2024/09/23/airbrb/avhouse.png" class title="avhouse"><img src="/2024/09/23/airbrb/inv.png" class title="inv"><p><strong>4. Booking a Listing</strong><br>Logged-in users can select a property and book it. Users choose a date range for their stay and confirm the booking. The booking is then processed by the backend.</p><p>Booking Functionality: Users select dates and submit a booking request.<br>View Booking Status: Users can check the status of their bookings (pending, accepted, or denied).</p><img src="/2024/09/23/airbrb/reserve.png" class title="reserve"><img src="/2024/09/23/airbrb/re.png" class title="reserve"><p><strong>5. Reviews &amp; Ratings</strong><br>Users can leave reviews for properties they have booked. This helps future users get a better sense of the property’s quality and the host’s reliability.</p><p>Leave a Review: After completing a booking, users can leave a rating and a comment for the property.<br>View Reviews: Other users can see reviews left by previous guests, including ratings and feedback.</p><img src="/2024/09/23/airbrb/review.png" class title="review"><p><strong>Advanced Features</strong><br><strong>1. Star Rating Breakdown</strong><br>When hovering over the star rating, a tooltip shows the percentage of reviews in each star category. This feature is similar to rating systems seen on e-commerce platforms.</p><p><strong>2. Profit Tracking</strong><br>For property hosts, the platform provides a profit tracking graph that shows the income earned from their listings over the past month. This allows hosts to monitor their earnings.</p><p><strong>3. YouTube Thumbnails</strong><br>To make property listings more engaging, hosts can use YouTube videos as the listing thumbnail. This gives potential renters a better sense of the property with a video tour.</p><p><strong>Development Process</strong><br>During development, one of the key challenges was ensuring that the frontend communicated efficiently with the provided backend API. I used React’s useState and useEffect hooks to manage state and fetch data dynamically. React Router was employed to handle navigation between different views without refreshing the page, providing a seamless user experience.</p><p>From a UI&#x2F;UX perspective, I focused on making the platform intuitive and responsive. The design was tested on different devices and screen sizes to ensure a smooth experience for both mobile and desktop users.</p><p><strong>Conclusion</strong><br>This project provided an in-depth learning experience in ReactJS, allowing me to build a fully functional, user-friendly application. From user authentication to booking and review systems, the AirBrB frontend offers a wide range of features. The added advanced functionality, like profit tracking and star rating breakdown, further enhances the user experience. If you have any questions about this project, feel free to reach out!</p>]]></content>
    
    
    <categories>
      
      <category>Frontend</category>
      
    </categories>
    
    
    <tags>
      
      <tag>html</tag>
      
      <tag>react</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Building FinTech Microservices for Data Analytics and Automated Machine Learning</title>
    <link href="/2024/09/22/automl/"/>
    <url>/2024/09/22/automl/</url>
    
    <content type="html"><![CDATA[<p><strong>Overview</strong><br>In the project, our team created four types of microservices for data collection, pre-processing, visualisation and modelling. These microservices are flexible and can be utilised to build data pipelines for different applications such as stock pricing prediction and weather forecasting. According to the client’s requirement, the microservices are just Python functions.</p><img src="/2024/09/22/automl/Overview.png" class title="Overview"><p><strong>Project Goals</strong></p><ol><li>All microservices must be developed in Python, reusable and well-documented.</li><li>All services must be executed by a common user Interface (UI) using the same workflow.</li><li>Build multiple microservices and combine microservices into a meaningful application (data pipeline: execute microservices in series). At least two data pipelines should be implemented. At least one microservice can be used by more than one data pipeline.</li><li>At least one data pipeline is related to AutoML applications.</li><li>Combine at least two different but related datasets in one data pipeline.</li><li>The microservices should be flexible enough to handle different inputs or generate different results by changing the parameters.<br><strong>System Architecture</strong><img src="/2024/09/22/automl/System_Architecture.png" class title="System Architecture"><strong>System Functionalities</strong><br><strong>Collection</strong></li><li>Data Retrieval from API<br>The microservice allows users to fetch data from external Yahoo Finance API. Users can<br>specify the API parameters, like ticker, start date and end date to retrieve the required data.<br>The microservice employs secure and efficient communication protocols to ensure the<br>confidentiality and integrity of the data during the retrieval process.</li><li>User upload dataset<br>To make a finance related model (e.g stock price prediction) usually requires years of data.<br>Fetching data from API every time could be time consuming. Hence, the collection<br>microservice also allows users to upload their own dataset for further processing.</li><li>Saving to Load Storage<br>Upon successful retrieval or upload, the data is securely stored in microservice local storage.<br>This ensures the availability and durability of the data for further processing.</li><li>Data Transformation into Pandas DataFrame<br>The retrieved data is transformed into a Pandas DataFrame, a powerful and widely used<br>data manipulation tool in Python. The transformation process is optimised for efficiency to<br>handle large datasets.</li><li>Local Copy for User Download<br>To enhance user experience, the microservice allows users to download a local copy of the<br>collected data. Users can choose the desired format (e.g., CSV, ADAGE) for the downloaded<br>file. The microservice ensures data integrity during the download process and provides<br>users with prompt feedback on the status of their download requests.</li></ol><p><strong>Pre-processing</strong></p><ol><li>Data Cleaning<br>The preprocessing microservice in our AutoML platform encompasses a broad range of<br>functionalities, specifically designed to tackle complex data quality issues. Within the data<br>cleaning section, two key features are prominently available: ‘Remove Null Values’ and<br>‘Remove Duplicates’. We have meticulously developed custom functions for these features,<br>aptly named remove_null and remove_duplicate, respectively. Users can effortlessly remove<br>null values and duplicates from their dataset with just a single click on the respective<br>buttons.</li></ol><img src="/2024/09/22/automl/Data_Cleaning_Buttons.png" class title="Data Cleaning Buttons"><ol start="2"><li>Data Filtering<br>The Data Filtering section of the platform empowers users to refine their dataset effectively<br>by offering two significant functionalities: filtering by date range and dropping columns. Users<br>can specify a start and end date to filter their data within a certain period, and they also have<br>the option to exclude specific data dimensions by dropping columns. This flexibility allows<br>users to concentrate on the data that is most relevant and crucial for their subsequent<br>analysis. To facilitate these features, we have developed custom functions named date_filter<br>and drop_column, specifically tailored to execute these tasks efficiently.</li></ol><ol start="3"><li>Data Transformation<br>The data transformation microservices focus on preparing and manipulating data for<br>analysis. This involves unpacking, aggregating, and filling missing data in time series.<br>3.1 Unpacking Dictionary Columns<br>The function is designed to expand dictionary-like data within a DataFrame column into<br>separate columns. This is crucial for extracting nested information, such as sentiments, from<br>a single column.<br>3.2 Average Column Calculation<br>For example, if the column is sentiment, then the function computes the average sentiment<br>for each date. It groups data by date and calculates the mean of each sentiment column,<br>which is essential for time-series analysis. In this case the daily sentiment averages can<br>indicate market trends.<br>3.3 Data Filling Strategies<br>The functions address missing data in time series. Forward filling replicates the last known<br>data point, and zero filling inserts zeros. These methods are vital for maintaining the<br>continuity of time series data.</li></ol><p><strong>Visualisation</strong></p><ol><li>Generation page<br>Data generation is an essential step before visualisation. Once the data has been loaded<br>and verified, they can be passed into the visualisation functions we create (will be discussed<br>later), producing intuitive candle plots and line plots. Our visualisation tools, such as the time<br>slider below the chart, further enhance the user experience, allowing users to focus on a<br>specific time frame to explore detailed changes in stock prices.<br>When integrating the “data generation” section of the report, we emphasise the technical<br>details of data preprocessing and loading, as well as how to turn this data into a visual table.<br>1.1 Data Collection<br>In order to improve the effectiveness and accuracy of our visualisation module, we have<br>introduced data generation steps in the AutoML platform. This step focuses on the<br>acquisition and pre-processing of raw stock data, through to the final loading and display.<br>The screenshot shown below is the data generation page on our platform, which shows the<br>stock data in tabular form.</li></ol><img src="/2024/09/22/automl/Data_Generation_Page.png" class title="Data_Generation_Page"><p>Our system is designed with reusability in mind. For example, the call to the render_template<br>function gives us the flexibility to use the same data table component for different data sets<br>and pages. This modular approach makes the platform easy to maintain and extend.<br>On the back end, the &#x2F;generation routing code handles the entire process from file deletion,<br>data reading, and template rendering. Error handling mechanisms are included in this<br>process to ensure robustness, for example, if the preprocessed file does not exist, we try to<br>delete it to avoid redundancy and potential data inconsistencies.</p><ol start="2"><li>Visualisation Page<br>In the modern FinTech space, data visualisation plays an important role in helping users<br>understand complex data. The purpose of this report is to detail the visualisation module in<br>our project, which focuses on converting stock market data into intuitive charts and graphs to<br>help users better understand the dynamics of the stock market.<br>Combined with data filtering and transformation steps, our visual microservices generate<br>graphs that reflect the most relevant data insights.</li></ol><img src="/2024/09/22/automl/Visualisation_Page.png" class title="Visualisation_Page"><p>We use the Plotly graph library to create intuitive charts based on filtered and transformed<br>data sets, such as candle charts and line charts, which visually show fluctuations and trends<br>in stock prices and market sentiment.</p><p><strong>Modelling</strong></p><ol><li>AutoML<br>The model microservices are designed to automate the process of forecasting time series<br>data using AutoGluon (Shchur et al., 2023). It primarily focuses on predicting future values<br>based on historical data, which is crucial in various domains like stock market analysis,<br>weather forecasting.</li><li>Data Preparation for AutoML<br>Function divides the dataset into training and testing sets based on a specified date column<br>and test rate. This is crucial for evaluating the model’s performance on unseen data.<br>Function standardises the column names, ensuring consistency and compatibility with the<br>AutoGluon library.</li><li>Model Training<br>Function utilises the TimeSeriesPredictor from AutoGluon, which trains a model to predict<br>future values of a specified target column for a given number of days ahead. This can be<br>used for training the model with training dataset, as well as refitting the model with full<br>dataset.</li><li>Logger<br>Helper class and functions for monitoring the progress of AutoML training, which count the<br>number of logs and smooth the percentage of progress bar.</li></ol><img src="/2024/09/22/automl/Training_Page.png" class title="Training_Page"><ol start="5"><li>Model Evaluation<br>Function generates leaderboards using the trained models on both the test and full datasets.<br>This provides insights into the model’s performance and helps in selecting the best model for<br>prediction.</li><li>Forecasting<br>Function makes predictions on new data, outputting a dataframe for future trends.</li></ol><img src="/2024/09/22/automl/Results_Page.png" class title="Results_Page"><p><strong>User Interface and User experience</strong><br>AutoML is designed by considering responsive, reusable and meaningful screens and<br>components. All the components and screens are responsive to desktop, tablet and mobile<br>devices.</p><ol><li>Responsiveness<br>The user interface of AutoML web app accommodates major platforms which can run web<br>apps including Desktop, Tablet and mobile below are the screenshots of the same<br>Here you can see that the buttons and the content changes its orientation and adapts to the<br>respective view.<br>The respective row layout in the desktop view changes into column</li></ol><img src="/2024/09/22/automl/Desktop_View.png" class title="Desktop_View"><img src="/2024/09/22/automl/Phone_and_Tablet_View.png" class title="Phone_and_Tablet_View"><ol start="2"><li>Common Components &#x2F; Adaptive Components<br>AutoML app heavily leverages Common and adaptive components at wherever necessary<br>to maintain consistency and code reuse for the code bases<br>Consistency: By employing common components, AutoML maintains a consistent look and<br>feel. This uniformity helps in maintaining the same user experience throughout the app while<br>ensuring that the user interface is coherent and intuitive. Moreover standardising practices<br>and conventions across different parts of the application.<br>Leveraging Code Reuse: The use of adaptive and common components promotes the<br>reuse of code leading towards more efficient development process, quicker turnaround times<br>for new features, and fewer errors, as well-tested components are reused rather than<br>creating new components for example the tables and buttons are using a shared under the<br>hoods the table which is displayed in 5 screens hence is the same component and the<br>buttons through the app<br>Easier to Maintain and Upgrade: Since our web app relies on reusable components,<br>maintaining and updating the application becomes simpler. Modifications or improvements<br>made to a common component automatically propagate to all parts of the application which<br>are using the same components resulting in reduction of time and effort needed to<br>implement changes across the application.<br>Adaptable and scalable: Adaptive components allow the application to be more flexible and<br>scalable. They can adjust to different contexts and requirements, making the application more robust and versatile. Like we have adapted the table component in all the five screens<br>and we can also scale the table to use either a data frame or a csv.</li></ol><img src="/2024/09/22/automl/Phone_and_Tablet_View_of_Progress_Bar.png" class title="Phone_and_Tablet_View_of_Progress_Bar"><ol start="3"><li>Preprocessing<br>3.1 Preprocessing Page Overview<br>The AutoML platform features a dedicated preprocessing page that enables users to easily<br>clean, filter, sort, and transform data for further analysis and modelling. The platform<br>seamlessly integrates front-end interactivity with robust back-end processing. The<br>user-friendly web interface comprises three clearly defined sections: Data Cleaning, Data<br>Filtering, and Data Preview, which show users the immediate impact of their preprocessing<br>actions.</li></ol><img src="/2024/09/22/automl/Preprocessing_Page_Overview.png" class title="Preprocessing_Page_Overview"><p>3.2 Data Preview<br>The Data Preview section displays the data table, where users can view the generated data<br>from the previous generation page and see the results after applying preprocessing features.<br>This section shows the immediate effect of their preprocessing actions, granting users<br>control over their data. Additionally, users have the option to select the number of entries<br>they wish to display and can search for keywords within the dataset using the provided<br>search box.</p><img src="/2024/09/22/automl/Data_Preview_Overview.png" class title="Data_Preview_Overview"><p>3.3 Data Sorting<br>To enhance user experience and interactivity in the UI, each column in the displayed table is<br>accompanied by arrows for sorting. An upward arrow next to a column name enables the<br>user to sort that column in ascending order, while a downward arrow facilitates sorting in<br>descending order. This feature allows users to quickly and efficiently organise their data in<br>the desired order, simply by clicking on the respective arrow beside each column header.</p><img src="/2024/09/22/automl/Data_Sorting_Arrows.png" class title="Data_Sorting_Arrows"><p>3.4 Preprocessing Results<br>The preprocessed data is ultimately displayed in a table in the Data Preview section, ready<br>for further visualisation, analysis, and modelling purposes. Users also have the option to<br>download the preprocessed data by clicking on the button labelled ‘Download Preprocessed<br>Data’.</p><img src="/2024/09/22/automl/Download_Prepr_cessed_Data_Button.png" class title="Download_Prepr_cessed_Data_Button"><ol start="4"><li>Visualisation<br>Users can choose to generate an OHLC chart or a line chart table by clicking a button on the<br>interface.</li></ol><img src="/2024/09/22/automl/OHLC_Chart.png" class title="OHLC_Chart"><p>4.1 OHLC chart<br>In the OHLC chart, you see the picture below</p><img src="/2024/09/22/automl/Graph_1.png" class title="Graph_1"><p>Each candlestick represents a day’s stock price, and when you hover over a particular day, it<br>will show the day’s date and the average stock price for that day.</p><img src="/2024/09/22/automl/Graph_2.png" class title="Graph_2"><p>On the top right of the picture you can see a row of function buttons. If you want to save the<br>image locally you can click on the camera button and the image will be saved in png format<br>on your computer, the pan button allows you to select the portion of the stock price date you<br>want to select then zoom in, the box select and lasso select allow you to select any area of<br>the image you want, there is also zoom in and zoom out functionality and the ability to reset<br>the entire image.</p><img src="/2024/09/22/automl/Function_Buttons.png" class title="Function_Buttons"><p>The sliders underneath allow you to shorten or zoom in on the time period you want to<br>observe. Of course, if you find the price changes of a certain period is interesting and you<br>want to analyse it quickly by selecting this period of the chart, you just need to select the part<br>of the chart you are interested in with your mouse and it will be zoomed in automatically on<br>the period you have selected, if you want to go back to the original chart, you just need to<br>double-click on the mouse and you will be able to return to the original chart.</p><img src="/2024/09/22/automl/Graph_3.png" class title="Graph_3"><p>4.2 Line chart<br>Similarly, all of the above features are also available on the line chart if you choose to use it,<br>which gives you a more visual picture of the trend of the selected stock over the time period.<br>Each type of chart is designed for specific user needs, whether it is to analyse in depth the<br>specific performance of a single stock or to observe the overall trend of the stock price.</p><img src="/2024/09/22/automl/Line_Chart.png" class title="Line_Chart"><p>Our visualisation module plays a key role in making stock market data analysis more intuitive<br>and user-friendly. It is not only suitable for professional investors, but also provides value to<br>ordinary stock market enthusiasts. In the future, we plan to continue optimising the module<br>by introducing more interactive features and chart types to meet a wider range of user<br>needs.</p><p><strong>Implementation Challenges</strong><br>High level framework vs Customised UI<br>In the project proposal, we planned to use Streamlit for front-end UI implementation.<br>Streamlit is an innovative and user-friendly Python library that simplifies the process of<br>creating interactive web applications for data science and machine learning. With Streamlit,<br>developers can transform data scripts into shareable web apps with just a few lines of code,<br>eliminating the need for extensive web development expertise.<br>However, as a high level UI framework, the simplicity comes with a cost being very difficult to<br>customise for use, it does not provide enough flexibility. For instance, in our AutoML<br>platform, users use our microservices to form a pipeline. Streamlit does not have a pipeline<br>like navigation bar. It only allows users to navigate using sidebars. It is a viable<br>implementation for users to navigate, but it is not a user friendly solution.<br>Hence, we created a new battle plan and decided to switch to Flask for the front-end. Flask<br>allows us to write customised HTML UI templates with JQuery. We made the decision after<br>the first demonstration, therefore, during the second sprint, we faced a challenge of<br>refactoring sprint one UI to Flask and finishing sprint two on time. To tackle the challenge, we<br>fully utilised the flexible week to have coding sessions to increase our work efficiency and<br>quality. Coding session helped us to quickly refactored the code to Flask and left us enough<br>time for sprint two development.<br>Team Programming Version Control<br>Team based projects are profoundly different from individual projects. In an individual<br>project, one person could code any part of the project in any way and any time preferred.<br>However, team based coding is more complicated. It requires all team members to adhere to<br>the correct version control method.<br>For most team members, it was the first time using JIRA for project management and<br>working on a real-life-like project. Members had no ideas how to utilise Git properly to control<br>code versions. For instance, one member needs to create a branch to make modifications or<br>develop new features, then create a Pull Request to merge changes into the main branch.<br>The correct workflow ensures new code has good quality and can be safely merged into the<br>main branch. At the beginning of the project, without using the correct Git workflow, the<br>coding progress was strongly influenced by the problem. Team members had to share and<br>check progress using team chat.<br>To resolve the problem, we hosted a meeting to familiarise ourselves with Git workflow. We<br>ensured everyone can use JIRA for user story and branch creation, know how to create Pull<br>Requests and approve then merge other’s Pull Requests. After the meeting, we successfully<br>resolved this challenge, we could maximise our efficiency to increase the project quality.</p><p><strong>Third Party Libraries</strong><br><em><strong>Flask</strong></em><br>Pallets. (n.d.). Pallets&#x2F;flask: The Python Micro Framework for building web<br>applications. GitHub. <a href="https://github.com/pallets/flask/">https://github.com/pallets/flask/</a><br>We used Flask for UI implementation. One standout feature about Flask is that it allows us to<br>create UI templates. In our AutoML platform, we need to use tables to visualise data to<br>users. With the help of UI components, we could implement one table template and reuse it<br>in different pages. This modular design not only enhances code readability but also<br>simplifies updates and modifications.<br>In our project, we used Flask for routing, each microservice has one endpoint, calling the<br>endpoint brings users to corresponding pages. We created two types of UI templates. The<br>first type is components, which are reusable minimum elements for User Interface. The<br>second type is pages, which contain components to form endpoint corresponding pages.<br><em><strong>Pandas</strong></em><br>Pandas-Dev. (n.d.). Pandas-dev&#x2F;Pandas: Flexible and powerful data analysis &#x2F;<br>manipulation library for python, providing labelled data structures similar to R<br>data.frame objects, statistical functions, and much more. GitHub.<br><a href="https://github.com/pandas-dev/pandas">https://github.com/pandas-dev/pandas</a><br>Pandas is an open-source Python library which is extensively used for data manipulation<br>and analysis. It provides data structures and functions to efficiently handle and analyse<br>structured data. We have utilised Pandas in our AutoML platform for various microservices<br>including collection, model, preprocessing, and even within our Flask app code.<br>Pandas plays a crucial role in our AutoML platform, due to its efficiency, adaptability, and<br>strong capabilities in handling data. Its features align seamlessly with the requirements of<br>data analysis and automated machine learning, making it an essential tool in our system.<br><em><strong>Plotly</strong></em><br>Plotly. (n.d.). Plotly&#x2F;plotly.py: The Interactive Graphing Library for Python this project<br>now includes Plotly Express!. GitHub. <a href="https://github.com/plotly/plotly.py">https://github.com/plotly/plotly.py</a><br>Plotly is an open source graphics library for creating interactive charts and data<br>visualisations. It supports several programming languages, including Python, R, Julia, and<br>JavaScript. Plotly makes it easy for users to build complex graphs, increasing the<br>intuitiveness of data analysis and user interaction.<br><em><strong>AutoGluon</strong></em><br>Shchur, O., Turkmen, C., Erickson, N., Shen, H., Shirkov, A., Hu, T., &amp; Wang, Y. (2023,<br>August 10). Autogluon-timeseries: Automl for probabilistic time series forecasting.<br>arXiv.org. <a href="https://arxiv.org/abs/2308.05566">https://arxiv.org/abs/2308.05566</a><br>AutoGluon is an AutoML Library for Image, Text, Time Series, and Tabular Data. In this<br>project we used the time series forecasting functionality.<br><em><strong>Yfinance</strong></em><br>yfinance: Download market data from Yahoo! Finance’s API. GitHub.<br><a href="https://github.com/ranaroussi/yfinance">https://github.com/ranaroussi/yfinance</a><br>Yfinance is an open-source publicly available API, it allows us to fetch financial data to<br>conduct research.<br>The API is easy to use, with a ticker, a start date and an end date, it allows users to fetch<br>stock data. The API is free to use and reliable.<br><strong>User Manual</strong><br><em><strong>Installation</strong></em></p><ol><li>Prerequisite: Before beginning the installation, ensure Python 3.10 is installed on your<br>system. This is an essential step for the proper functioning of the application.</li><li>Clone the Repository: The first step involves cloning the repository to your local machine.<br>This action copies all the necessary files you need to start setting up the application.</li><li>Navigation to Project Directory: Once the repository is cloned, open a terminal or<br>command prompt. Navigate to the project directory by executing the command:<br>cd app<br>This will change your current directory to the project folder.</li><li>Installing Required Packages: In the project directory, you need to install several<br>dependencies. This is done using pip, Python’s package installer, with the following<br>command:<br>pip install -r requirements.txt<br>This command reads the ‘requirements.txt’ file and installs all the packages listed there.</li><li>Install AutoGluon: AutoGluon is an essential component of our application. Follow the<br>installation guide available at AutoGluon’s official installation page<br>(<a href="https://auto.gluon.ai/stable/install.html">https://auto.gluon.ai/stable/install.html</a>) to install this package.<br><strong>Running the Application</strong></li><li>Activating the Python Environment: Before running the application, ensure the Python<br>environment where you installed the dependencies is activated.</li><li>Starting the Application: To start the application, run the Flask app with debugging mode<br>enabled. This can be done by executing the following command in the terminal:<br>flask –app app.py –debug run<br>This command starts the Flask server with the application, allowing you to access it through<br>a web browser.<br><strong>Understanding the Project Structure</strong><br>The project organised for easy navigation and modification:<br>● app Directory: Contains the main application and modules for various microservices.<br>○ helpers: Includes utility scripts and helper functions that assist in various<br>tasks.<br>○ microservices : Contains services for data collection, modelling,<br>preprocessing, results, and visualisation.<br>○ ui: Hosts user interface components and HTML pages for different<br>functionalities.<br>● app.py: This is the main Python file used to run the Flask application.<br>● setup&#x2F;Requirements.txt: Lists all the dependencies required for the project.<br>● README.md: Contains detailed setup instructions and additional information about<br>the project.<br>Usage</li><li>After landing on the welcome page, click on the ‘Get Started’ button.</li></ol><img src="/2024/09/22/automl/Welcome_Page.png" class title="Welcome_Page"><ol start="2"><li>The user will be directed to the Collection page of the AutoML platform. The user can<br>choose to fetch data from an API or upload a local dataset by clicking on the respective<br>buttons. To fetch data from the API, use ‘AAPL’ as the ticker, input preferred start and end<br>dates, and use ‘demo’ as the API Token.</li></ol><img src="/2024/09/22/automl/Collection_Page.png" class title="Collection_Page"><ol start="3"><li>The fetched data is displayed on the Generation page. The user can choose to download<br>the data or download ADAGE by clicking on the respective buttons</li></ol><img src="/2024/09/22/automl/Data_Generation_Page.png" class title="Data_Generation_Page"><ol start="4"><li>On the Preprocessing page, the user can clean the data by removing null values and<br>duplicates using the ‘Remove Null Values’ and ‘Remove Duplicates’ buttons respectively.<br>Furthermore, the data can be filtered based on a specified date range and unnecessary<br>columns can be excluded using the provided data filtering features.<br>The preprocessed data is displayed in a table format in the data preview section. The data<br>can be sorted in either ascending or descending order using the arrows provided beside the<br>column names in the table. The user can also choose to download the preprocessed data by<br>clicking the ‘Download Preprocessed Data’ button.</li></ol><img src="/2024/09/22/automl/Preprocessing_Page_Overview.png" class title="Preprocessing_Page_Overview"><img src="/2024/09/22/automl/Data_Preview_Overview.png" class title="Data_Preview_Overview"><img src="/2024/09/22/automl/Download_Prepr_cessed_Data_Button.png" class title="Download_Prepr_cessed_Data_Button"><ol start="5"><li>After the user is satisfied with pre-processed data, the user can navigate to the<br>visualisation page. Two plots about the dataset will be generated. The first one is the OHLC<br>chart, which shows the user the key stock price of a day (e.g. low, high). The second one is<br>an average line chart, which shows the user the average stock price for each day.</li></ol><img src="/2024/09/22/automl/Average_Line_Chart.png" class title="Average_Line_Chart"><ol start="6"><li>After the dataset is prepared, the user can navigate to the modelling page. In the<br>modelling page, the user will have a dropdown list of each column in the dataset. The user<br>can choose one column they want to predict and number of days to predict. Finally, with one click on the start button, the AutoML will handle the rest (e.g. train Time Series Model to<br>predict).</li></ol><img src="/2024/09/22/automl/Modelling_page.png" class title="Modelling_page"><p>While the models are being trained, the user will see a progress bar to know when the<br>training will be finished and be able to see results.<br>7. Ultimately, when the training is finished, the user can navigate to the result page and view<br>detailed results from each model. Models will be sorted in performance order from the most<br>accurate to the least accurate.<br>Moreover, the user can also view predicted results from each model by choosing the model<br>from the dropdown bar. Then click the ‘start predicting’ button to show all predictions.</p><img src="/2024/09/22/automl/Results_Page1.png" class title="Results_Page1">]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>automl</tag>
      
      <tag>Python</tag>
      
      <tag>microservices</tag>
      
      <tag>data pipeline</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Adding a Geographical Coordinate Data Type to PostgreSQL</title>
    <link href="/2024/09/21/DBMS/"/>
    <url>/2024/09/21/DBMS/</url>
    
    <content type="html"><![CDATA[<p><strong>Introduction</strong></p><p>Geographical coordinates play a crucial role in various applications, ranging from geolocation services to geographical data analysis. However, working with geographical data types in relational databases can sometimes be limiting without the right tools. This blog explores a project designed to implement a custom geographical coordinate data type, GeoCoord, in PostgreSQL using C. The project demonstrates the process of adding a new SQL type and how to integrate it into the PostgreSQL backend.</p><p><strong>Background</strong></p><p>The aim of this project was to create a new SQL data type, GeoCoord, which could store and handle geographical coordinates (latitude and longitude) along with their directional markers (N, S, E, W) and an associated location name. Additionally, various comparison operators and a unique hashing function were implemented to allow indexing and efficient querying of the new data type.</p><p>The GeoCoord type’s design focuses on simplicity and efficiency. By allowing geographical coordinates to be stored directly within PostgreSQL, developers can perform operations such as comparison, distance calculation, and even conversion to a more human-readable Degree-Minute-Second (DMS) format directly within SQL queries.</p><p><strong>Structure of the GeoCoord Data Type</strong></p><p>The GeoCoord data type is implemented in C and represents a custom PostgreSQL datatype. The actual structure for the GeoCoord type consists of:</p><p>Latitude and longitude as floating-point values.<br>Latitude and longitude directional markers (N, S, E, W).<br>A variable-length character array to store the name of the location.</p><img src="/2024/09/21/DBMS/typedef.png" class title="typedef"><p><strong>I&#x2F;O Functions</strong></p><p>Two main functions handle the input and output of the GeoCoord data type in PostgreSQL.</p><p>Input Function (gcoord_in): This function parses a string input into the GeoCoord structure. The input consists of a location name followed by geographical coordinates (latitude and longitude). The function ensures that the input is valid and assigns the corresponding values to the structure.</p><img src="/2024/09/21/DBMS/gcoord_in.png" class title="gcoord_in"><p>Output Function (gcoord_out): This function converts the GeoCoord structure back into a string format when retrieving the data. The result includes the location name followed by latitude and longitude in degrees.</p><img src="/2024/09/21/DBMS/gcoord_out.png" class title="gcoord_out"><p><strong>Comparison Operators</strong></p><p>In addition to input and output functions, the project implements several comparison operators (&lt;, &lt;&#x3D;, &#x3D;, &gt;&#x3D;, &gt;), enabling users to perform comparisons between two GeoCoord instances. These operators compare geographical coordinates first by latitude and then by longitude. The following function handles internal comparison:</p><img src="/2024/09/21/DBMS/gcoord_com.png" class title="gcoord_com"><p><strong>Additional Features: Time Zone Comparison and DMS Conversion</strong></p><p>Two additional utility functions were implemented to extend the functionality of the GeoCoord type:</p><p>Time Zone Comparison (~ and !~ operators): These functions compare the longitude values of two GeoCoord instances to determine if they fall within the same time zone.</p><img src="/2024/09/21/DBMS/time.png" class title="time"><p>DMS Conversion: The convert2dms function converts the geographical coordinates from decimal format to Degrees-Minutes-Seconds (DMS) format, which is often more intuitive for humans to understand.</p><img src="/2024/09/21/DBMS/convert.png" class title="convert"><p><strong>Indexing Functions</strong></p><p>For performance optimization, the GeoCoord type supports both B-tree and hash indexing. This allows the efficient querying of large datasets involving geographical coordinates. The gcoord_cmp function is used for B-tree indexing, while the gcoord_hash function is used for hash indexing.</p><img src="/2024/09/21/DBMS/index.png" class title="index"><p><strong>Implementation in PostgreSQL</strong></p><p>The implementation in PostgreSQL involves creating the GeoCoord type and linking it with the corresponding C functions. This is done via SQL statements that define the new data type, operators, and functions.</p><img src="/2024/09/21/DBMS/create_type.png" class title="create_type"><p>With these functions, developers can use the GeoCoord type in PostgreSQL queries, leveraging features like comparison, time zone checking, and DMS conversion.</p><p><strong>Conclusion</strong></p><p>This project successfully demonstrates how to extend PostgreSQL with custom data types, providing enhanced functionality for geographical coordinates. The GeoCoord type allows efficient storage, comparison, and manipulation of geographic data directly within SQL queries, making it a valuable tool for applications that handle location-based data. By combining powerful C backend code with the flexibility of PostgreSQL, this project highlights the potential of custom data types in relational databases.</p><p>For those looking to work with geographical data, or to create custom PostgreSQL data types, this project serves as a comprehensive example of how to do so from scratch.</p>]]></content>
    
    
    <categories>
      
      <category>DBMS</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Database</tag>
      
      <tag>SQL</tag>
      
      <tag>PostgreSQL</tag>
      
      <tag>C#</tag>
      
      <tag>Memory management</tag>
      
      <tag>Function pointers</tag>
      
      <tag>File I/O</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Implementing Relational Operators and Memory Buffer Management</title>
    <link href="/2024/09/20/DBMS2/"/>
    <url>/2024/09/20/DBMS2/</url>
    
    <content type="html"><![CDATA[<p><strong>Introduction</strong></p><p>Database systems require efficient ways to manage data storage and retrieval, especially when dealing with large datasets. Key operations like selection and join must be optimized to minimize input&#x2F;output (I&#x2F;O) operations, particularly when constrained by limited memory buffers. In this project, I implemented two critical relational operators, selection and join, within the context of buffer management using a Clock-Sweep replacement policy. This blog will walk through the implementation process, highlighting key challenges and solutions.</p><p><strong>Memory Buffer Management with Clock-Sweep</strong></p><p>Memory buffers play a pivotal role in how data pages are read from disk and maintained in memory. Given a finite number of buffer slots, pages need to be managed carefully. The Clock-Sweep algorithm was implemented to handle this, where pages in the buffer are “cycled through” to determine which ones to evict when new pages need to be loaded.</p><p><em><strong>Clock-Sweep Algorithm</strong></em><br>The Clock-Sweep algorithm is simple yet effective:</p><ol><li>Each page in the buffer has a pin (indicating if the page is being used) and a usage counter (indicating recent access).</li><li>When a page is requested but not found in the buffer, the algorithm sweeps through the buffer to find a page with a pin of 0 and a usage of 0, which can be replaced.<br>In the code, request_page and release_page are responsible for handling page requests and releasing them. Here’s how a page is fetched using the Clock-Sweep strategy:</li></ol><img src="/2024/09/20/DBMS2/request.png" class title="request"><p>This ensures that pages are managed efficiently in the buffer, avoiding excessive reads from disk.</p><p><strong>Selection Operator</strong><br>The selection operator (sel) is a basic relational operation that filters rows (tuples) based on a condition. The challenge in this implementation was efficiently reading the pages from disk, checking each tuple for the condition, and storing the matching tuples.</p><p>In the code, the sel function iterates through pages of a table and checks the attribute at the specified index. If the value matches the condition, the tuple is added to the result:</p><img src="/2024/09/20/DBMS2/sel.png" class title="sel"><img src="/2024/09/20/DBMS2/sel2.png" class title="sel2"><img src="/2024/09/20/DBMS2/sel3.png" class title="sel3"><p>This function efficiently handles selection, even for large datasets, by using buffer management to minimize disk reads.</p><p><strong>Join Operator</strong><br>The join operator is more complex, as it requires combining rows from two tables based on a condition. In this project, two approaches were implemented:</p><ol><li>Sort-Merge Join when buffer slots are sufficient to load all pages into memory.</li><li>Nested Loop Join when the number of buffer slots is limited.</li></ol><p><em><strong>Sort-Merge Join</strong></em><br>When enough buffer slots are available, a sort-merge join is performed. Both tables are sorted by the join attributes, and then the sorted tables are merged:</p><img src="/2024/09/20/DBMS2/join1.png" class title="join1"><img src="/2024/09/20/DBMS2/join2.png" class title="join2"><img src="/2024/09/20/DBMS2/join3.png" class title="join3"><img src="/2024/09/20/DBMS2/join4.png" class title="join4"><p>In this approach, the sort_tuples function sorts the tuples using qsort, and the tables are merged based on the sorted order, reducing the complexity compared to the nested loop join.</p><p><em><strong>Nested Loop Join</strong></em><br>When memory is constrained, a nested loop join is employed, where one table is iterated in an outer loop, and the other table in an inner loop:</p><img src="/2024/09/20/DBMS2/join5.png" class title="join5"><img src="/2024/09/20/DBMS2/join6.png" class title="join6"><img src="/2024/09/20/DBMS2/join7.png" class title="join7"><img src="/2024/09/20/DBMS2/join8.png" class title="join8"><p>This method is less efficient but necessary when memory is limited.</p><p><strong>Efficient File Management</strong><br>File I&#x2F;O is a costly operation, so managing file pointers effectively is crucial. In this project, the number of open files was limited to a configurable value, and files were opened and closed as needed. The functions log_open_file and log_close_file were used to track file operations and ensure that the number of open files never exceeded the limit.</p><p><strong>Logging and Debugging</strong><br>Throughout the project, extensive logging was used to track key operations such as page reads, releases, and file operations. Functions like log_read_page and log_release_page were instrumental in debugging buffer and I&#x2F;O issues. This provided a clear view of when pages were being loaded and replaced in the buffer.</p><p><strong>Conclusion</strong><br>The implementation of relational operators and memory buffer management in this project highlighted the importance of efficient buffer usage and file handling in a database system. The Clock-Sweep strategy proved effective for managing pages, while the combination of sort-merge join and nested loop join provided flexibility based on available memory. Through careful buffer management and optimization, the project successfully demonstrated how database operations can be handled efficiently even with limited resources.</p><p>For those interested in database system internals, this project provides a practical look at key concepts like buffer management, file I&#x2F;O optimization, and relational operator implementation.</p>]]></content>
    
    
    <categories>
      
      <category>DBMS</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Database</tag>
      
      <tag>SQL</tag>
      
      <tag>PostgreSQL</tag>
      
      <tag>C#</tag>
      
      <tag>Memory buffer management</tag>
      
      <tag>File system management</tag>
      
      <tag>Sort-merge join and hash join</tag>
      
      <tag>Database file management</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Predict student performance from game play</title>
    <link href="/2024/09/19/predict-perform/"/>
    <url>/2024/09/19/predict-perform/</url>
    
    <content type="html"><![CDATA[<p><strong>Introduction</strong></p><p>With the increasing adoption of game-based learning in education, students are now engaging with dynamic and interactive educational games that enhance the learning experience. However, many platforms still lack efficient ways to track individual knowledge progress. The “Predict Student Performance from Game Play” project aimed to bridge this gap by predicting student responses to in-game questions based on gameplay data, contributing to the research on knowledge tracking in educational games. This was a Kaggle competition task that focused on time-series prediction, using cutting-edge machine learning and deep learning models.</p><p><strong>Project Objective</strong></p><p>The objective was simple yet challenging: predict whether players would answer 18 specific questions correctly (Yes or No) by analyzing time-series data generated from an online educational game. The evaluation metric used was the F1-score, a harmonic mean of precision and recall that is particularly useful in binary classification tasks.</p><p><strong>Challenges</strong></p><p>The competition required real-time predictions using Kaggle’s time-series API, and hidden test data was released in groups, with no access to future data points. Predictions were required at three key checkpoints: levels 4, 12, and 22. This design meant that models had to be both accurate and responsive to ensure success across all questions.</p><p><strong>Methodology</strong></p><p>To tackle this task, the team developed 18 distinct models—one for each question. These models were trained based on different level groupings:</p><p>Levels 0–4 to predict questions 1 to 3 (3 models)<br>Levels 5–12 to predict questions 4 to 13 (10 models)<br>Levels 13–22 to predict questions 14 to 18 (5 models)<br>This division allowed the models to focus on data relevant to specific stages in the game, improving their accuracy. All feature engineering processes were grouped by session_id and level_group, enabling the models to track the progress of individual players.</p><img src="/2024/09/19/predict-perform/method.png" class title="method"><p><strong>Feature Engineering</strong></p><p>Feature engineering played a pivotal role in this project, as it provided the data needed for accurate predictions. The following steps were taken:</p><p>Calculated the sum, mean, and standard deviation of each event name.<br>Analyzed actions related to event names (binary encoding: 1 or 0).<br>Calculated unique values in text-related columns such as fqid and text_fqid.<br>Extracted statistical information (quantile, mean, min, max, std) from numerical columns.<br>Used advanced techniques like Principal Component Analysis (PCA) and K-means clustering to generate additional features.<br>In total, nearly 600 features were crafted to improve the models’ predictive power.</p><img src="/2024/09/19/predict-perform/feature.png" class title="feature"><img src="/2024/09/19/predict-perform/feature2.png" class title="feature2"><img src="/2024/09/19/predict-perform/feature5.png" class title="feature5"><img src="/2024/09/19/predict-perform/feature6.png" class title="feature6"><p><strong>Models and Hyperparameter Tuning</strong></p><p>Two types of models were employed in this project:</p><p>Gradient Boosted Decision Trees (GBDT): XGBoost and LightGBM, which are known for their high performance in structured data problems.<br>Deep Learning: Wide and Deep Networks, designed to capture both linear and complex interactions in the data.<br>To fine-tune the models, the team used GroupKFold cross-validation and Optuna, a state-of-the-art hyperparameter optimization library. Model ensembling (combining XGBoost and LightGBM) was also implemented to improve overall performance.</p><img src="/2024/09/19/predict-perform/XGBoost.png" class title="XGBoost"><img src="/2024/09/19/predict-perform/model-combination.png" class title="model-combination"><img src="/2024/09/19/predict-perform/deep_networks.png" class title="deep_network"><img src="/2024/09/19/predict-perform/train.png" class title="train"><p><strong>Results</strong></p><p>The model performance was evaluated using local cross-validation with GroupKFold (n_split&#x3D;3). Here’s a summary of the key results:</p><p>Traditional feature engineering (XGBoost): F1-score &#x3D; 0.685<br>After adding PCA: F1-score &#x3D; 0.687<br>After adding K-means clustering: F1-score &#x3D; 0.688<br>XGBoost model: F1-score &#x3D; 0.688<br>LightGBM model: F1-score &#x3D; 0.686<br>Wide and Deep Networks: F1-score &#x3D; 0.631<br>Model combination (XGBoost and LightGBM): F1-score &#x3D; 0.689<br>The final choice was a model combination of XGBoost and LightGBM, which achieved an impressive F1-score of 0.688 on Kaggle’s hidden test data.</p><p><strong>Conclusion</strong></p><p>This project demonstrated the power of feature engineering and model ensembling in improving the performance of machine learning models. The use of advanced techniques like PCA, K-means clustering, and Optuna for hyperparameter optimization proved to be highly effective in a real-time, game-based learning environment. While deep learning models underperformed compared to GBDT models, the combination of XGBoost and LightGBM delivered the best results. This approach offers promising directions for future research into knowledge tracking for game-based learning platforms.</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
      <tag>XGBoost</tag>
      
      <tag>Lightgbm</tag>
      
      <tag>GroupKFold</tag>
      
      <tag>Feature Engineering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image Classification and Object Detection Using Faster R-CNN</title>
    <link href="/2024/09/18/computer-vision/"/>
    <url>/2024/09/18/computer-vision/</url>
    
    <content type="html"><![CDATA[<p><strong>Introduction</strong><br>In recent years, with the rapid development of computer vision and deep learning technology, image classification and object detection have made remarkable progress, and have been widely used in many fields. One important application is wildlife monitoring, where the identification and tracking of various species plays a vital role in conservation efforts.<br>In this context, our research focuses on the challenging task of image classification and target detection on the “Penguins vs Turtles” dataset [1] provided by the well-known platform Kaggle.<br>The main goal of our research is twofold: First, using advanced machine learning algorithms, images are accurately classified into two different categories, namely penguins and turtles, based on their visual characteristics.  Successful completion of these missions will not only contribute to the scientific understanding of wildlife, but will also pave the way for intelligent systems that can effectively assist conservationists and researchers in monitoring and protecting these species.<br>The “Penguins vs Turtles” dataset is an outstanding resource curated for the Kaggle community and contains a large number of annotated images showing penguins and turtles in their natural habitat. Each image is carefully labeled to indicate whether penguins or turtles are present in it, facilitating supervised learning methods for classification and object detection. The diversity and complexity of this dataset presents both exciting challenges and great opportunities for developing robust and accurate models.<br>In this article, we describe the methods and results of handling the above tasks. We delve into the complex details of our chosen deep learning architecture, dataset preprocessing techniques, and optimization strategies. In addition, we conducted a comprehensive experiment to analyze the performance of our model and compare it with the most advanced methods. By sharing our insights and findings, we hope to contribute to common knowledge in the field of image classification and target detection, while demonstrating the importance of these applications in the field of wildlife conservation.</p><p><strong>Literature Review</strong><br>  Image classification and object detection are basic tasks in computer vision and have many applications in wildlife monitoring, autonomous driving and robotics. Over the years, researchers have developed various techniques to address these challenges, each with its own unique advantages and limitations. In this section, we will review the relevant literature on image classification and object detection.</p><p><em><strong>image classification</strong></em><br>  Image classification is designed to assign a label or category to an input image from a predefined set of categories. Convolutional neural networks (CNNs) have played a key role in revolutionizing image classification because of their ability to automatically learn hierarchical features from data. Earlier work, such as AlexNet[2] and VGG[3], demonstrated the effectiveness of deep CNNS in large-scale image classification tasks, achieving top performance in well-known benchmarks such as ImageNet. Since then, many improvements have been made to improve the efficiency and accuracy of the image classification model. GoogLeNet[4], ResNet[5], and DenseNet[6] are notable architectures that introduce techniques such as jump joins, residual learning, and dense joins, respectively, to deal with disappearing gradients and help train extremely deep networks.</p><p><em><strong>object detection</strong></em><br>  Object detection involves identifying and locating multiple objects of interest in an image. Traditional object detection methods, such as sliding window methods and Haar-like features, have limitations for dealing with complex scenes with different sizes and orients.<br>Region-based convolutional neural networks (R-CNN) [7] work by introducing the concept of regional proposals generated by selective searches and then processing these regions through CNN-based classifications. The method involves extracting regional proposals from input images and feeding them into CNNS one by one for feature extraction and classification. Although effective, RCNN needs to process a large number of regional proposals independently and has a high computational overhead, making it less practical in real-time applications.<br>Fast R-CNN[8] is an improvement on RCNN that addresses its efficiency issues by introducing several key improvements. Fast R-CNN no longer handles each regional proposal independently, but shares the convolutional features of all regional proposals. This shared feature extraction significantly reduces computational time and increases the overall speed of the model. In addition, a Region of Interest (ROI) pooling layer is introduced to align features extracted from irregularly shaped region proposals into a fixed-size feature map, which is convenient for classification in the fully connected layer. The Fast R-CNN achieved significant improvements in accuracy and efficiency over its predecessor.<br>Building on the success of Fast R-CNN, Faster R-CNN[9] proposes a ground-breaking and innovative approach to integrating Regional proposal networks (RPNS) into the target detection process. RPN generates regional proposals as an integral part of the end-to-end training process, making the model a fully convolutional structure and significantly faster than previous methods. Faster R-CNN uses a two-stage approach: First, the RPN generates candidate regions with bounding box proposals, which are then fed into the subsequent target detection network for classification and bounding box coordinate fine-tuning. RPNS are trained to predict anchor boxes that tightly enclose instances of objects, and these predictions are used to sort and filter regional proposals based on the likelihood of containing objects. Integrating RPN with Faster R-CNN proved to be highly effective, showing state-of-the-art performance across multiple target detection benchmarks.</p><p><strong>Methods</strong><br>In this section, we detail the methods used to solve image classification and target detection tasks in the “Penguins vs Turtles” dataset. Both missions use the Faster R-CNN architecture, which performs well in target detection benchmarks and has real-time processing capabilities.<br>Faster R-CNN is a major advance in the field of object detection, combining the advantages of regional proposal networks and convolutional neural networks in an end-to-end framework, greatly reducing the amount of computation by sharing the feature layer, thus increasing the detection speed of the algorithm. Key components of Faster R-CNN include the Regional Proposal Network (RPN) and the subsequent target detection network.<br>RPN generates regional proposals by predicting anchor boxes that tightly surround potential target instances. These anchor frames serve as reference areas with different scales and aspect ratios to propose candidate areas containing targets. The RPN shares convolution features with the target detection network to form a unified, computationally efficient model. The object detection network inputs the RPN-generated area proposal, classiifies it to determine the presence of the target, and performs regression to fine-tune the bounding box coordinates. Use ROI pooling to adapt and align RPN feature maps to generate fixed-size feature vectors for efficient and accurate classification and regression. </p><img src="/2024/09/18/computer-vision/Faster_RCNN_algorithm.png" class title="Faster_RCNN_algorithm"><p><em><strong>Backbone network</strong></em><br>Faster R-CNN can adopt a variety of backbone feature extraction networks, such as VGG, Resnet, Xception, etc. In this paper, Resnet50 network is adopted. Resnet network is a kind of deep residual network, which directly introduces the data output of a certain layer of the previous layers into the input part of the later data layer, which means that a part of the content of the later feature layer will be linearly contributed by a certain layer of the previous. Deep residual network can effectively solve the problem that the learning efficiency decreases and the accuracy cannot be effectively improved due to the deepening of network depth. </p><img src="/2024/09/18/computer-vision/Resnet.png" class title="Resnet"><p>Resnet50 has two basic blocks, namely Conv Block and Identity Block. The dimensions of input and output of Conv Block are different, so they cannot be connected continuously. Its function is to change the dimension of the network. Can be connected in series to deepen the network.</p><p><em><strong>region proposal networks</strong></em><br>RPN(Region Proposal Networks) networks are regional nomination networks that take an image of any size as input and output a set of rectangular target proposals with targeted scores.<br>To generate the region proposal, we slide a small network onto the convolutional feature map of the output of the last shared convolutional layer. This small network takes an n×n spatial window as input, where n is a parameter. Each sliding window is mapped to a low-dimensional feature, which is then activated by ReLU[10]. This feature is entered into two parallel fully connected layers, one for regression and one for classification. In this article, we use n &#x3D; 3. </p><img src="/2024/09/18/computer-vision/RPN.png" class title="RPN"><p>Each grid has three size prediction boxes, and each prediction box has three ratios, respectively 1:1, 1:2 and 2:1. Then each prediction box is classified by grid, and the prior box and the real box respectively correspond to a central coordinate point and a group width. The weight in the RPN network is adjusted by learning the difference between the prior box and the real box. Finally, it is used to predict the position of objects. It is worth noting that since the small network operates in a sliding window fashion, the fully connected layer is shared across all spatial locations. This architecture is naturally implemented by an n×n convolution layer, followed by two parallel 1×1 convolution layers.</p><p><em><strong>Loss function</strong></em><br> To train the RPN, we assign each anchor a binary class label (indicating whether it is an object or not). We assign positive category tags to two types of anchors: (i) An anchor with the highest IoU overlap with any ground-truth bounding box, or (ii) an anchor with an IoU overlap greater than 0.7 with any ground-truth bounding box. Note that a single ground-truth bounding box may assign positive category labels to multiple anchor points. In general, the second condition is sufficient to determine the positive sample; However, in order to ensure that positive samples can be found in some rare cases, we still use the first condition. For all non-positive sample anchor points with an IoU ratio less than 0.3 to the ground-truth bounding box, we assign them as negative category labels. An anchor point that is neither a positive nor a negative sample contributes nothing to the training goal.<br>According to the above definition, we train the RPN by minimizing an objective function that follows the multitask loss in Fast R-CNN. Specifically, the objective function is defined as:</p><img src="/2024/09/18/computer-vision/loss_function.png" class title="loss_function"><p>For bounding boxes, we use the following four parameterization methods:</p><img src="/2024/09/18/computer-vision/boundin_box.png" class title="boundin_box"><p>Where x, y, w, and h represent the center coordinates and width and height of the bounding box, respectively. The variables x, , and represent prediction bounding boxes, anchor bounding boxes, and ground-truth bounding boxes, respectively (the same goes for y, w, and h). This can be seen as a bounding box regression from an anchor bounding box to a nearby ground-truth bounding box.</p><p><strong>Experimental results</strong><br>In this section, we describe the experimental setup used to evaluate the performance of the “Faster R-CNN” model for image classification and target detection on the “Penguins vs Turtles” dataset. We describe data set statistics, training configurations, evaluation metrics, and conduct a comprehensive analysis of the results.</p><p><em><strong>Data preprocess</strong></em><br>Due to the limitation of the computer hardware device (GPU), the image is uniformly adjusted to the input size of 256x256 and normalized during data preprocessing.<br>In addition, in target detection task, the bounding box annotation format given by “Penguins vs Turtles” data set is[, , , ], which is not easy to draw the bounding box. Therefore. We set the target of the model to [, , , ]，and the label is converted once during data preprocessing.</p><p><em><strong>Train configuration</strong></em><br>For the construction of “Faster R-CNN” model, we use PyTorch, a popular deep learning framework, and use ResNet50 pre-trained model as the backbone network for feature extraction. The models were trained on the “Penguins vs Turtles” dataset, and the RPN and the target detection network were jointly optimized using the SGD optimizer. The optimizer is set to a learning rate of 0.001, trains 20 epochs, and saves the epoch that works best based on the evaluation metrics on the validation set. </p><img src="/2024/09/18/computer-vision/loss.png" class title="loss"><p><em><strong>Results and Analysis</strong></em><br>For image classification, the “Faster R-CNN” model achieves a higher Accuracy rate. The change curves of accuracy and F1-score in the training process</p><img src="/2024/09/18/computer-vision/accuracy.png" class title="accuracy"><p>In the training process of the image classification network, we save the epoch with the best classification effect according to F1-score. The evaluation indexes of the training set and verification set of this model in the dataset “Penguins vs Turtles” are shown in Table 1:<br>          Accuracy  PrecisionRecall  F1-score<br>Train set0.9940  1.0000  0.9880      0.9940<br>Valid set0.9306  0.9697  0.8889  0.9275<br>Table1<br>The confusion matrix of the performance effect of Faster R-CNN on Penguins and Turtles images alone</p><img src="/2024/09/18/computer-vision/matrix.png" class title="matrix"><p>For target detection: The “Faster R-CNN” model shows better performance. During the training of the network, we saved the epoch with the best effect on target recognition according to the average IoU of the model on the verification set. The IoU of the training set and the verification set of this model on the “Penguins vs Turtles” data set and the distance between the center of the prediction box and the center of the real box are shown in Table 2.<br>            IoU     Distance<br>Train set0.8971  3.6045<br>Valid set0.8775  4.9678</p><p><strong>Discussion</strong><br>Our experimental results for image classification and target detection on the “Penguins vs Turtles” dataset using the “Faster R-CNN” network perform well, demonstrating the effectiveness and potential of this approach. In this section, we discuss experimental results, method performance, and possible causes of observed failures.<br>The “Faster R-CNN” model shows excellent performance in image classification and target detection tasks. For image classification, the model achieves an accuracy of 0.9306 on the validation set, effectively distinguishing penguins and turtles based on visual features. The high accuracy indicates that the model has the ability to accurately predict and correctly classify images. In the target detection task, the “Faster R-CNN” model achieved an average IoU score of 0.8775 on the validation set. It reflects the model’s ability to accurately locate and classify penguins and turtles in images.<br>However, the real-time performance of “Faster R-CNN” is poor, because the model is larger, the parameters are more numerous, and the calculation process is more complicated. This model has excellent accuracy, but also makes the calculation rate is relatively low, the calculation time is long, no matter in the process of training or reasoning, it needs to consume a long time.<br>In addition, in this experiment, in the classification task, the model pays more attention to the posture of the object. As shown in Figure, when the movement and posture of the penguin are the same as that of the turtle, the model will recognize the penguin as a turtle, but will not recognize the penguin from the consideration of skin color and background.</p><img src="/2024/09/18/computer-vision/tur.png" class title="tur"><img src="/2024/09/18/computer-vision/pen.png" class title="pen"><p>In the task of target recognition, although the model has been able to detect the location of the target well and generate a reasonable detection frame for it, in terms of details, the model is more inclined to generate a small target detection frame. As shown in the Figure, the model does not completely frame the edge part of the hand and foot of the object.</p><img src="/2024/09/18/computer-vision/pen1.png" class title="pen"><img src="/2024/09/18/computer-vision/tur1.png" class title="tur"><p>For the above problems, we have also made corresponding explanations. Some possible reasons for the error include: a. Limited data sets: Despite the diversity of the Penguins vs Turtles dataset, it is possible that underrepresentation of certain species can lead to misclassification. b. Small or distant objects: Detection of small or distant objects can be challenging, especially if the object features are not obvious, which can lead to missing or inaccurate bounding boxes for detection.</p><p><strong>Conclusion</strong><br>In this study, we explore the application of the “Faster R-CNN” architecture to image classification and target detection tasks on the “Penguins vs Turtles” dataset. The results show that the Faster R-CNN model performs well in both tasks, achieving high accuracy for image classification tasks and achieving significant average IoU scores for object detection tasks. This work has important implications for wildlife monitoring and conservation efforts, as accurately classifying and locating penguins and turtles provides valuable insights into the field of wildlife ecology.<br>Effectiveness: Faster R-CNN shows robustness and efficiency in image classification and target detection tasks. Combining area suggestion network (RPN) with object detection network helps to achieve real-time capability and high accuracy of the model.<br>Limitations: Models face challenges in detecting small or distant objects, as well as environmental occlusion. This can lead to potentially erroneous negative results and inaccuracies in bounding box predictions.<br>Recommendations for future work: To further improve the performance of the Faster R-CNN model and address the observed limitations, future work could focus on the following areas:<br>Data enhancement and collection: Enhance the dataset with more diverse transformations and collect additional data with a balanced distribution of categories and various environmental conditions to enhance the generalization of the model.<br>Hyperparameter tuning: Performing extensive hyperparameter tuning allows you to optimize model performance and adjust parameters for better results.<br>Integrated approach: By combining predictions from multiple models or architectures, you can improve overall performance and enhance robustness.<br>Transfer learning: Using models pre-trained on larger and more diverse datasets for transfer learning helps models gain knowledge from related tasks and improve performance.<br>Advanced object Detection architectures: Studying other state-of-the-art object detection architectures, such as YOLO or SSDS, may provide valuable insights and potentially improve performance.</p>]]></content>
    
    
    <categories>
      
      <category>Computer Vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Faster R-CNN</tag>
      
      <tag>python</tag>
      
      <tag>Image Classification</tag>
      
      <tag>Object Detection</tag>
      
      <tag>Computer Vision</tag>
      
      <tag>PyTorch</tag>
      
      <tag>Convolutional Neural Networks (CNN)</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
